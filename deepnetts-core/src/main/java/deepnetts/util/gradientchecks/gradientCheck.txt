Zasto pucaju gradient checks?

Testiraj Gradient check za LINEARNU REGRESIJU
od ovoga treba napraviti junit testove

--------------------------------

U ovoj iteraciji se bavim samo tacnoscu implementacije, a numericku tacnost cu videti u buducnosti

LINEARNA REGRESIJA BEZ SKRIVENOG SLOJA
    + 1 input i 1 output neurona - simple linear regression. Generisi csv sa random podacima sa linearnim trendom. Prolazi gradient check
    + 2 i vise input i jednog output neurona (multiple linear regression) - napravi csv fajl sa 2 ulaza to je 3d funkcija -ravan!
        imam samo jedan slucaj kada nije dobra at weight[0,0]gradient:-0.0014925301 numeric grad: -0.0014156103relative error:0.05153651 >>>>>>>>> not good!
        verovatno zbog neke numericke greske. Zato sto je mnogo mali ulaz jedino tu je x =0.02 !!!!!!!

    Vidi dal ima promene ako se ukljuce negativne vrednosti

    da li ima smisla sa vide output neurona, mislim da ne, ali mogu da probal

LINEARNA REGRESIJA SA SKRIVENIM SLOJEM - ovo sledece
    dodaj jedan skriveni sloj sa linear, 
    +sigmoid
        ---- Layer 1-----------------------------------------------------------------------------
        At weight[0,0]gradient:0.004907094 numeric grad: 0.0048428774relative error:0.013086474 >>>>>>>>> not good!
        For input:0.11811108
    + tanh (ima gresku na jednom weightu)
        ---- Layer 2-----------------------------------------------------------------------------
        At weight[0,0]gradient:-0.0017253136 numeric grad: -0.0016763806relative error:0.028361782 >>>>>>>>> not good!
        For input:0.346584
    + relu
    + leaky rely - brljavi na vis emesta
        ---- Layer 1-----------------------------------------------------------------------------
        At weight[0,0] gradient:-0.001964912 numeric grad: -0.0020489097 relative error:0.04099625 >>>>>>>>> not good!
        For input:0.25512284

        ---- Layer 2-----------------------------------------------------------------------------
        At weight[0,0] gradient:2.9700037E-4 numeric grad: 3.3527613E-4 relative error:0.114161894 >>>>>>>>> not good!
        For input:0.25512284

    ---- Layer 1-----------------------------------------------------------------------------
    At weight[0,0] gradient:-0.002188341 numeric grad: -0.0020861626 relative error:0.046692185 >>>>>>>>> not good!
    For input:0.2909815
    ---- Layer 2-----------------------------------------------------------------------------
    At weight[0,0] gradient:1.6868062E-4 numeric grad: 1.8626451E-4 relative error:0.09440281 >>>>>>>>> not good!
    For input:0.2909815

    ---- Layer 1-----------------------------------------------------------------------------
    At weight[0,0] gradient:-8.306882E-4 numeric grad: -7.8231096E-4 relative error:0.058237568 >>>>>>>>> not good!
    For input:0.11811108
    ---- Layer 2-----------------------------------------------------------------------------
    At weight[0,0] gradient:7.105691E-4 numeric grad: 6.7055225E-4 relative error:0.0563166 >>>>>>>>> not good!
    For input:0.11811108
    ---- Layer 1-----------------------------------------------------------------------------
    At weight[0,0] gradient:-0.0023213744 numeric grad: -0.0023841858 relative error:0.02634502 >>>>>>>>> not good!
    For input:0.2935165
    ---- Layer 2-----------------------------------------------------------------------------
    At weight[0,0] gradient:1.6930861E-4 numeric grad: 1.8626451E-4 relative error:0.09103133 >>>>>>>>> not good!
    For input:0.2935165



    TODO:
        !!! ako dodam 2 hidden neurona sa linearnom funkcijom onaj nulti nije dobar i to uvek [0,0] tu je garantovano neki zez jer dosta masi!!!
        isto i ako dodam 5!!! uvek [0,0] je pogresan
        moguc problem je sto za input ide u granu za 3d layere!!!  tamo verovatno im aneki zez
        ili mozda falsira klasa Tensor za neki get set
        testiraj ti i uporedi dense layer kako se poredi sa num pyom i tensorflow-om
        ubaci ove casove u junit test

        ili sam nedge zamenio rows i cols?
Da u Dense layeru red 303!!!!! zamenio sam row i col
sigurno ima jos neka ovakva
red 301 nedostajao je deltaCol gradients.set(inRow, inCol, inDepth, deltaCol, grad);

        a izleda da random generator ne crta dobro tacke jer nema negativnih!!! sumnjiv je. I isproveravaj linear regresion primer jer sa onaj 1-1 radi besprekorno graient check

        ---- Layer 1-----------------------------------------------------------------------------
        At weight[0,0] gradient:-0.048434183 numeric grad: 0.16286969 relative error:1.29738 >>>>>>>>> not good!
        For input:0.346584

   ne proveravam bias u gradient checku, mada je on sigurno dobar jer uzima deltu

--------------------------------------------------------------------------------------------------------------------------------------

TESTIRAJ I BOSTON DATASET  

Uradi gradient check i za klasifikaciju 
i napravi jednu test klasu (apstraktnu?) koja se moze koristiti za razne probleme



KLASIFIKACIJA 
    XOR
    Neka binarna klasifikacija? DAAA 
    
    Logistic regression

    IRIS
        uglavnom dobar ali ima nekih koji nisu!!! Zasto? Izgleda zbog nrmalizacije???!!!
        izgleda da kada su ulazi visoki baca NaN!!! ulazi u saturaciju.... gradijent nije tacan

    MOzda data setovi sa oblicima?

-------------------------------

Kada resim gradijente za linearnu regresiju da vidim da li dobro radi predikciju za linearnu regresiju. Trenutno to prilicno lose radi...

MERE PERFROMANSI ZA LINEARNU REGRESIJU
MAE
MSE
R2
Sa opisima
